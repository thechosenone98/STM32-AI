{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14824f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5dd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the datatype that we saved in our binary files\n",
    "dt = np.dtype([('x', np.double), ('y', np.double), ('z', np.double), ('timestamp', np.ulonglong)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66dbfa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(os.getcwd()).joinpath(r\"./AccelerometerData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8865fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all file in the directory (it is important to note that the filename you have used will be the class names from now on)\n",
    "_, _, filenames = next(os.walk(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e6eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_1.bin\n",
      "Error_2.bin\n",
      "Normal.bin\n",
      "Off.bin\n",
      "            Mode       x       y       z      timestamp\n",
      "0        Error_1  0.0546 -1.0842 -0.1560  1619640596650\n",
      "1        Error_1 -0.0468 -1.0374 -0.0312  1619640596652\n",
      "2        Error_1 -0.1326 -1.0452  0.0000  1619640596654\n",
      "3        Error_1 -0.1326 -1.0062  0.0312  1619640596656\n",
      "4        Error_1 -0.0624 -0.9282  0.0468  1619640596658\n",
      "...          ...     ...     ...     ...            ...\n",
      "1305814      Off  0.0234 -0.9984  0.0000  1619980693003\n",
      "1305815      Off  0.0156 -1.0062 -0.0702  1619980693005\n",
      "1305816      Off  0.0078 -0.9906 -0.0234  1619980693007\n",
      "1305817      Off  0.0234 -0.9984 -0.0546  1619980693009\n",
      "1305818      Off  0.0234 -0.9984 -0.0546  1619980693011\n",
      "\n",
      "[1305819 rows x 5 columns]\n",
      "['Error_1', 'Error_2', 'Normal', 'Off']\n"
     ]
    }
   ],
   "source": [
    "# This function convert every item in the numpy array to a list instead of a tuple\n",
    "def convert(item):\n",
    "    return np.asarray(item)\n",
    "# Iterate through the file and add a column to them corresponding to their class names\n",
    "data = pd.DataFrame()\n",
    "class_names = []\n",
    "for file in filenames:\n",
    "    print(file)\n",
    "    filepath = data_path.joinpath(file)\n",
    "    class_name = file.split('.')[0]\n",
    "    class_names.append(class_name)\n",
    "    class_data = np.fromfile(filepath, dtype=dt)\n",
    "    # Fromfile as this weird way of loading data in so we need to convert\n",
    "    # everything back to a list and reconvert to numpy array with the corresponding data type.\n",
    "    class_dataframe = pd.DataFrame(class_data)\n",
    "    class_dataframe.insert(0, \"Mode\", class_name)\n",
    "    data = data.append(class_dataframe, ignore_index=True)\n",
    "print(data)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06179d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our dataframe, we can start augmenting it and patching the missing data\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22ef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that our data is all well in a DataFrame we can start creating our X and y arrays\n",
    "# We want to create a 25x3 image (the 3 is simply x, y, z data of each sample)\n",
    "# from our input data so that we can give it to a CNN model\n",
    "# These model are made to work really well on images. Our data might not seem like an image but trust me for a minute\n",
    "# and you'll understand why we do it this way later.\n",
    "\n",
    "# We'll begin by creating some helful functions\n",
    "\n",
    "# Here are some library we will need to perform some of the modification\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "# This function gives us the window of data to put in each of our samples\n",
    "# each time you call it the window steps forward (by N step) through our data but it's width remains the same\n",
    "def window_gen(data, window_size, step=1):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield start, start + window_size\n",
    "        start += step\n",
    "        \n",
    "# This function splits our data into 25x3 chunks and also create an array with the corresponding label\n",
    "def split_data(data, window_size=10, step=10):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels= np.empty((0))\n",
    "    for (start, end) in tqdm(window_gen(data['timestamp'],window_size, step)):\n",
    "        x = data['x'][start:end]\n",
    "        y = data['y'][start:end]\n",
    "        z = data['z'][start:end]\n",
    "        # check to make sure that if the window size is not a multiple of our data, \n",
    "        # we simply discard the end (because it won't have the right lenght)\n",
    "        #if(len(data['timestamp'][start:end]) == window_size):\n",
    "        try:\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data['Mode'][start:end])[0][0])\n",
    "        except:\n",
    "            pass\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9ed4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65291it [20:32, 52.98it/s] \n"
     ]
    }
   ],
   "source": [
    "X, y = split_data(data, window_size = 25, step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b7a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# We also need to one hot encode our output labels for the training\n",
    "y_categorical = np.asarray(pd.get_dummies(y), dtype=np.int8)\n",
    "print(y_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "156297bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these arrays to avoid having to recompute this lengthy step\n",
    "with open(\"training_data.npz\", \"wb\") as output_file:\n",
    "    np.savez(output_file, X=X, y=y, y_categorical=y_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0868ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reload the data from the file run these line:\n",
    "X = np.array([])\n",
    "y = np.array([])\n",
    "y_categorical = np.array([])\n",
    "with open(\"training_data.npz\", \"rb\") as input_file:\n",
    "    npzdata = np.load(input_file)\n",
    "    X = npzdata['X']\n",
    "    y = npzdata['y']\n",
    "    y_categorical = npzdata['y_categorical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1c9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an additionnal step we need to reshape our data to add an extra dimension for tensorflow (i.e the number of channel)\n",
    "X_reshaped = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31247ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our data seperated in a data array and a label array we can make a training set and a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce03b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import required for Tensorflow2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Let's now define a function that will create our model (not train it, only create it)\n",
    "def create_model(input_shape, nb_classes):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(4, (3,3), input_shape=(*input_shape, 1), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 1),padding='same'))\n",
    "    model.add(layers.Conv2D(8, (5,3), input_shape=(*input_shape, 1), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 1),padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(25, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model((25, 3), len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac03675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 25, 3, 4)          40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 3, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 3, 8)          488       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 3, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 25)                4225      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 4,857\n",
      "Trainable params: 4,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16d6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1429/1429 [==============================] - 12s 7ms/step - loss: 0.5556 - accuracy: 0.7723 - val_loss: 0.0082 - val_accuracy: 0.9983\n",
      "Epoch 2/10\n",
      "1429/1429 [==============================] - 9s 7ms/step - loss: 0.0255 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "1429/1429 [==============================] - 9s 7ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0037 - val_accuracy: 0.9990\n",
      "Epoch 4/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0181 - accuracy: 0.9951 - val_loss: 0.0089 - val_accuracy: 0.9970\n",
      "Epoch 5/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0129 - accuracy: 0.9968 - val_loss: 0.0153 - val_accuracy: 0.9986\n",
      "Epoch 7/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 9.3950e-04 - val_accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
      "Epoch 9/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0022 - val_accuracy: 0.9994\n",
      "Epoch 10/10\n",
      "1429/1429 [==============================] - 10s 7ms/step - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b5b5b5c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37feba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f621ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.5145514e-08 3.4300822e-33 9.9999988e-01 2.0309684e-27]\n",
      " [4.2951089e-17 0.0000000e+00 1.5053535e-03 9.9849463e-01]\n",
      " [1.0000000e+00 1.6129544e-11 9.3287794e-16 0.0000000e+00]\n",
      " ...\n",
      " [4.7285201e-14 1.0000000e+00 5.8667609e-37 0.0000000e+00]\n",
      " [8.7341050e-09 2.0901463e-34 1.0000000e+00 4.9927849e-23]\n",
      " [3.5331534e-17 0.0000000e+00 1.4865758e-03 9.9851340e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61e5dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal' 'Off' 'Error_1' ... 'Error_2' 'Normal' 'Off']\n",
      "['Normal' 'Off' 'Error_1' ... 'Error_2' 'Normal' 'Off']\n"
     ]
    }
   ],
   "source": [
    "# Convert prediction to class label and compare them to their expected values by creating a confusion matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(class_names)\n",
    "# Convert our prediction and the expected y_test back to class namespace\n",
    "y_predict_class = lb.inverse_transform(y_predict)\n",
    "y_test_class = lb.inverse_transform(y_test)\n",
    "print(predict_class)\n",
    "print(y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10e989c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4884    1    0    0]\n",
      " [   0 4857    0    0]\n",
      " [   1    0 4849    2]\n",
      " [   0    0    0 4993]]\n"
     ]
    }
   ],
   "source": [
    "# Now print the confusion matrix using the predicted class and the actual class contained in y_test\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test_class, y_predict_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8decc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's pretty good :) Let's now save our model and get converting (we need to convert it to c to run it on the STM32)\n",
    "model.save(\"anomaly_detection.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
